---
author: "Carlos Correa Íñiguez (<c.correainiguez@uandresbello.edu>)"
date: "2024-10-15"
format: 
  revealjs:
    css: unab.css
    slide-number: true
    transition: slide
    theme: beige
    auto-stretch: true 
output: html
output-file: index.html
---

### Unidad III: Análisis utilizando aprendizaje supervisado

- Clasificación Bayesiana y Máquinas de Soporte Vectorial (SVM)

---

### Objetivo de la Clase

- **Comprender** los fundamentos de la clasificación Bayesiana y su aplicación práctica.

- **Aplicar** los algoritmos de Máquinas de Soporte Vectorial (SVM) para la clasificación de datos.

- **Evaluar** las ventajas y desventajas de cada enfoque en diferentes escenarios de minería de datos.

---

### Introducción

1. Método práctico para inducir modelos probabilísticos y razonar sobre nuevos datos. Permite calcular la probabilidad asociada a cada hipótesis, ofreciendo una ventaja frente a otras técnicas.

2. Proporciona un marco para analizar variadas técnicas de aprendizaje y minería de datos, en el ambito de la descripción y clasificación.

---

### Ejemplo: Sistema de Recomendación en Inversiones

Como ejemplo de la ventaja que supone poder dar la probabilidad asociada a la clasificación, piénsese en un sistema de recomendaciones para invertir en bolsa.

- A partir de unos datos de entrada sobre un determinado producto, el sistema nos recomienda si invertir o no.

- Si no invertimos, no perdemos nada, pero si invertimos podremos multiplicar nuestra inversión o perderla parcial o totalmente.

---

- Supongamos que consultamos acerca de dos productos diferentes, **P1** y **P2**.

- Si el sistema no trata con incertidumbre, podría decir *SI* para ambos productos, lo que podría llevarnos a diversificar la inversión.

- Si el sistema usa un método bayesiano, la salida podría ser:

  - **P1**: *SI* con probabilidad 0.9, *NO* con probabilidad 0.1.
  - **P2**: *SI* con probabilidad 0.52, *NO* con probabilidad 0.48.

Sin duda alguna, el encargado de tomar la decisión preferiría tener esta información a la proporcionada por un sistema no bayesiano.

---

### Teorema de Bayes

- El Teorema de Bayes, propuesto por el filósofo inglés Thomas Bayes en 1760, permite calcular distribuciones condicionales.

- Este teorema proporciona la probabilidad de que ocurra un evento A, dado que ya ha ocurrido un evento B. La fórmula es:
$$ 
P(A|B) = \frac{{P(A) \cdot P(B|A)}}{{P(B)}}
$$

Donde:
  - **P(A|B)**: Probabilidad a posteriori.
  - **P(B|A)**: Verosimilitud.
  - **P(A)**: Probabilidad a priori.

---

### Clasificación Bayesiana: Naïve Bayes

- Suposición: todos los atributos son independientes conocido el valor de la clase.

- Si los eventos **A** y **B** son independientes, entonces se cumple que:

$$
P(A|B) = P(A) \quad \text{y} \quad P(B|A) = P(B)
$$

- El modelo de clasificación con redes bayesianas se basa en la suposición de que todos los atributos son independientes, conocido el valor de la variable clase.

---

### Representación Gráfica de una Red Bayesiana

- **Modelo probabilístico** con un único nodo raíz (la clase), y todos los atributos son nodos hoja que tienen como único padre a la variable clase.

- [Hernández et al., 2004]

![Representación de Red Bayesiana](Imagen1.png)

---

### Probabilidad Condicional con Múltiples Predictores

Para más de un predictor:

Sean $x_1$ y $x_2$ dos predictores de la clase $C$, la probabilidad condicional se define como:

$$
P(C|x_1, x_2) = \frac{P(x_1, x_2|C) \cdot P(C)}{P(x_1, x_2)} = \frac{P(x_1|C) \cdot P(x_2|C) \cdot P(C)}{P(x_1, x_2)}
$$

Para tres predictores:

$$
P(C|x_1, x_2, x_3) = \frac{P(x_1, x_2, x_3|C) \cdot P(C)}{P(x_1, x_2, x_3)} 
$$
$$
\frac{P(x_1, x_2, x_3|C) \cdot P(C)}{P(x_1, x_2, x_3)} = \frac{P(x_1|C) \cdot P(x_2|C) \cdot P(x_3|C) \cdot P(C)}{P(x_1, x_2, x_3)}
$$

---

### Ejemplo de Naive Bayes

Para entender mejor cómo funciona Naive Bayes, consideremos un conjunto de datos con 1500 observaciones y tres clases de salida:

- **Gato**
- **Loro**
- **Tortuga**

Las variables predictoras son categóricas (verdadero o falso):

- Nadar
- Alas
- Color verde
- Dientes afilados

---

### Resumen de Clases

**Clase de Gatos**:
- 450 de 500 (90%) de gatos pueden nadar
- 0 gatos tienen alas
- 0 gatos son de color verde
- Todos los 500 gatos tienen dientes afilados

**Clase de Loros**:
- 50 de 500 (10%) de loros pueden nadar
- Todos los 500 loros tienen alas
- 400 de 500 (80%) son de color verde
- Ningún loro tiene dientes afilados

**Clase de Tortugas**:
- Las 500 tortugas pueden nadar
- 0 tortugas tienen alas
- 100 de 500 (20%) son de color verde
- 50 de 500 (10%) tienen dientes afilados

---

### Clasificación con Naive Bayes

Con estos datos, clasifiquemos la siguiente observación en una de las clases de salida (gato, loro o tortuga) usando Naive Bayes.

### Observación:
- Nadar: Verdadero
- Color Verde: Verdadero

El objetivo es predecir si el animal es un **gato**, **loro** o **tortuga** basándonos en estas variables.

---

### Verificación: ¿Es un Gato?

Para comprobar si el animal es un gato, calculamos:

$$
\small P(\text{Gato} | \text{Nadar}, \text{Verde}) = \frac{P(\text{Nadar}|\text{Gato}) \cdot P(\text{Verde}|\text{Gato}) \cdot P(\text{Gato})}{P(\text{Nadar}, \text{Verde})}
$$

Sustituyendo valores:

$$
\small P(\text{Gato} | \text{Nadar}, \text{Verde}) = \frac{0.9 \cdot 0 \cdot 0.333}{P(\text{Nadar}, \text{Verde})} = 0
$$

---

### Verificación: ¿Es un Loro?

Para comprobar si el animal es un loro, calculamos:

$$
\small P(\text{Loro} | \text{Nadar}, \text{Verde}) = \frac{P(\text{Nadar}|\text{Loro}) \cdot P(\text{Verde}|\text{Loro}) \cdot P(\text{Loro})}{P(\text{Nadar}, \text{Verde})}
$$

Sustituyendo valores:

$$
\small P(\text{Loro} | \text{Nadar}, \text{Verde}) = \frac{0.1 \cdot 0.8 \cdot 0.333}{P(\text{Nadar}, \text{Verde})} = \frac{0.0264}{P(\text{Nadar}, \text{Verde})}
$$

---

### Verificación: ¿Es una Tortuga?

Para comprobar si el animal es una tortuga, calculamos:

$$
P(\text{Tortuga} | \text{Nadar}, \text{Verde}) = \frac{P(\text{Nadar}|\text{Tortuga}) \cdot P(\text{Verde}|\text{Tortuga}) \cdot P(\text{Tortuga})}{P(\text{Nadar}, \text{Verde})}
$$

Sustituyendo valores:

$$
P(\text{Tortuga} | \text{Nadar}, \text{Verde}) = \frac{1 \cdot 0.2 \cdot 0.333}{P(\text{Nadar}, \text{Verde})} = \frac{0.0666}{P(\text{Nadar}, \text{Verde})}
$$

---

### Verificación: ¿Es una Tortuga?

Para comprobar si el animal es una tortuga, calculamos:

$$
P(\text{Tortuga} | \text{Nadar}, \text{Verde}) = \frac{P(\text{Nadar}|\text{Tortuga}) \cdot P(\text{Verde}|\text{Tortuga}) \cdot P(\text{Tortuga})}{P(\text{Nadar}, \text{Verde})}
$$

Sustituyendo valores:

$$
P(\text{Tortuga} | \text{Nadar}, \text{Verde}) = \frac{1 \cdot 0.2 \cdot 0.333}{P(\text{Nadar}, \text{Verde})} = \frac{0.0666}{P(\text{Nadar}, \text{Verde})}
$$

---


### Conclusión

Para todos los cálculos, el denominador es el mismo, es decir, $P(\text{Nadar}, \text{Verde})$.
$$
P(\text{Tortuga} | \text{Nadar}, \text{Verde}) = 0.0666 / P(\text{Nadar}, \text{Verde})
$$
$$
P(\text{Loro} | \text{Nadar}, \text{Verde}) = 0.0264 / P(\text{Nadar}, \text{Verde})
$$

Dado que $P(\text{Tortuga} | \text{Nadar}, \text{Verde})$ es mayor que $P(\text{Loro} | \text{Nadar}, \text{Verde})$, podemos predecir correctamente que el animal es una **Tortuga**.

---

### Algoritmo de Naive Bayes

1. Encontrar $P(C_i)$, calculando el total de la i-ésima clase en el total de datos de entrenamiento.
2. Calcular la $P(x_t|C_i)$ para cada atributo o predictor de los datos de entrenamiento $X$.
3. Calcular la $P(X|C_i) = \prod_{t=1}^{n} P(x_t|C_i)$.
4. Calcular la $P(C_i|X)$.
5. Seleccionar la mayor probabilidad para clasificar los nuevos datos.

---

### Ejemplo: Diagnóstico de Sepsis usando Naive Bayes

Vamos a usar un ejemplo para ilustrar cómo funciona la clasificación Naive Bayes. El ejemplo utilizado es el diagnóstico de sepsis. Supongamos que hay dos predictores de sepsis: la tasa respiratoria y el estado mental.

---

### Tabla de Verosimilitudes

La siguiente tabla muestra las verosimilitudes para diagnosticar sepsis usando la tasa respiratoria y el estado mental. Los datos provienen de un conjunto de entrenamiento.

|               | Tasa Respiratoria        | Estado Mental          | Total |
|---------------|--------------------------|------------------------|-------|
|               | Rápida     | Lenta        | Alterado    | Normal   |       |
| **Sepsis**    | 15/20      | 5/20         | 17/20       | 3/20     | 20    |
| **No-Sepsis** | 5/80       | 75/80        | 3/80        | 77/80    | 80    |
| **Total**     | 20/100     | 80/100       | 20/100      | 80/100   | 100   |

---

### Probabilidades Previas

Las probabilidades previas para sepsis y no-sepsis son:

- $P(\text{sepsis}) = 20/100 = 0.2$
- $P(\text{no-sepsis}) = 80/100 = 0.8$

---

### Probabilidades de Verosimilitud

Las probabilidades de verosimilitud para los diferentes predictores son:

- $P(\text{tasa respiratoria rápida}|\text{sepsis}) = 15/20 = 0.75$
- $P(\text{tasa respiratoria lenta}|\text{sepsis}) = 5/20 = 0.25$
- $P(\text{tasa respiratoria rápida}|\text{no-sepsis}) = 5/80 = 0.0625$
- $P(\text{tasa respiratoria lenta}|\text{no-sepsis}) = 75/80 = 0.9375$

- $P(\text{estado mental alterado}|\text{sepsis}) = 17/20 = 0.85$
- $P(\text{estado mental normal}|\text{sepsis}) = 3/20 = 0.15$
- $P(\text{estado mental alterado}|\text{no-sepsis}) = 3/80 = 0.0375$
- $P(\text{estado mental normal}|\text{no-sepsis}) = 77/80 = 0.9625$

---

### Aplicando Naive Bayes

Queremos clasificar un paciente con una tasa respiratoria lenta y un estado mental alterado. Según la regla de clasificación de máxima verosimilitud, calculamos solo el numerador de la ecuación de Bayes.

La verosimilitud de sepsis dado una tasa respiratoria lenta y un estado mental alterado es:

$$
\tiny P(\text{sepsis}|\text{tasa respiratoria lenta} \cap \text{alterado}) = P(\text{tasa respiratoria lenta}|\text{sepsis}) \times P(\text{alterado}|\text{sepsis}) \times P(\text{sepsis})
$$

---

### Cálculo de Sepsis

Sustituyendo los valores en la fórmula:

$$
P(\text{sepsis}|\text{lenta}, \text{alterado}) = 0.25 \times 0.85 \times 0.2 = 0.0425
$$

---

### Cálculo de Sepsis

Sustituyendo los valores en la fórmula:

$$
P(\text{sepsis}|\text{lenta}, \text{alterado}) = 0.25 \times 0.85 \times 0.2 = 0.0425
$$

---

### Clasificación Bayesiana: Ventajas

- Funciona mejor que otros modelos o algoritmos, bajo el supuesto de independencia entre los predictores.
- Requiere una pequeña cantidad de datos de entrenamiento para estimar los datos de prueba.
- Es fácil de implementar.

---

### Clasificación Bayesiana: Desventajas

- Asume implícitamente que todos los atributos son mutuamente independientes; en la vida real, es casi imposible que obtengamos un conjunto de predictores completamente independientes.
- Si la variable categórica tiene una categoría en el conjunto de datos de prueba que no se observó en el conjunto de entrenamiento, el modelo asignará una probabilidad de 0 y no podrá hacer una predicción (frecuencia cero).

---

### Clasificadores Basados en Redes Bayesianas (RB)

- **TAN (Tree Augmented Naïve Bayes)**: permite ciertas dependencias entre los atributos, asumiendo una red bayesiana con forma de árbol.
- **BAN (Bayesian Network Augmented Naive Bayes)**: similar a TAN, permite iniciar la red como un NB y luego agregar arcos con un algoritmo de aprendizaje.
- **Redes Bayesianas**: aprenden una red incluyendo todas las variables (clase y atributos) para clasificar.

---

### Aplicaciones de Clasificación Bayesiana

- **Predicción en tiempo real**.
- **Predicción de clases múltiples**: bien conocido por su función en problemas de múltiples clases.
- **Clasificación de texto**: usada ampliamente debido a mejores resultados en problemas de varias clases y su regla de independencia.

---

### Otras Aplicaciones

- **Filtrado de correo no deseado**: se usa ampliamente en la detección de spam.
- **Análisis de opinión**: utilizado en el análisis de redes sociales para identificar sentimientos positivos y negativos.
- **Sistemas de recomendación**: para filtrar información y predecir si un usuario desea un recurso determinado o no.

---

### Ejemplo Scikit-learn Naive Bayes

Los métodos Naive Bayes son un conjunto de algoritmos de aprendizaje supervisado basados en la aplicación del teorema de Bayes con la suposición "ingenua" de independencia condicional entre cada par de características dado el valor de la variable de clase. El teorema de Bayes establece la siguiente relación, dada una variable de clase \(y\) y un vector de características dependientes \(\mathbf{x_1, \dots, x_n}\):

$$
P(y | \mathbf{x}) = \frac{P(y) P(\mathbf{x} | y)}{P(\mathbf{x})}
$$

Utilizando la suposición de independencia condicional, podemos simplificar esta relación a:

$$
P(y | \mathbf{x}) \approx P(y) \prod_{i=1}^{n} P(x_i | y)
$$

---

### Ejemplo Scikit-learn Naive Bayes

Ya que $P(\mathbf{x})$ es constante, podemos utilizar la siguiente regla de clasificación:

$$
\hat{y} = \underset{y}{\operatorname{argmax}} \ P(y) \prod_{i=1}^{n} P(x_i | y)
$$

Usamos la estimación de **Máxima A Posteriori (MAP)** para estimar $P(y)$ y $P(x_i | y)$, donde $P(y)$ es la frecuencia relativa de la clase $y$ en el conjunto de entrenamiento.

---

### 1.9.1. Gaussian Naive Bayes

El GaussianNB implementa el algoritmo Naive Bayes Gaussiano para clasificación. Se asume que las características siguen una distribución Gaussiana:

$$
P(x_i | y) = \frac{1}{\sqrt{2\pi\sigma_y^2}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2}\right)
$$

Los parámetros $\mu_y$ y $\sigma_y$ se estiman utilizando máxima verosimilitud.

---

### 1.9.1. Gaussian Naive Bayes (Codigo Python Scikit-learn)

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)

gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)

print("Número de puntos mal clasificados de un total de %d puntos: %d" % (X_test.shape[0], (y_test != y_pred).sum()))
```

---

### 1.9.2. Multinomial Naive Bayes

El **MultinomialNB** implementa el algoritmo Naive Bayes para datos distribuidos de forma multinomial. Este método es clásico para clasificación de texto, donde los datos suelen representarse como recuentos de vectores de palabras.

#### Fórmula de Multinomial Naive Bayes

El parámetro $\theta_{y,i}$ es la probabilidad de que la característica $i$ aparezca en una muestra perteneciente a la clase $y$, estimado como:

$$
\theta_{y,i} = \frac{N_{y,i} + \alpha}{N_y + \alpha n}
$$

Donde:

- $N_{y,i}$ es el número de veces que la característica \(i\) aparece en una muestra de la clase \(y\).
- $N_y$ es el recuento total de todas las características para la clase \(y\).
- $\alpha$ es un parámetro de suavizado.

---

### 1.9.3. Complement Naive Bayes

El **ComplementNB** es una adaptación del Naive Bayes multinomial que es particularmente adecuado para conjuntos de datos desequilibrados. Utiliza estadísticas del complemento de cada clase para calcular los pesos del modelo.

#### Fórmula de Complement Naive Bayes

La estimación de los pesos del modelo se calcula con la siguiente fórmula:

$$
\hat{\theta}_{c,i} = \frac{\alpha_i + \sum_{j: y_j \neq c} d_{i,j}}{\alpha + \sum_{j: y_j \neq c} \sum_k d_{k,j}}
$$

#### Parámetros de la Fórmula

- $\hat{\theta}_{c,i}$: Estimación del peso del modelo para la clase \(c\) y la característica \(i\).
- $\alpha_i$: Parámetro de suavizado.
- $d_{i,j}$: Frecuencia de la característica \(i\) en el documento \(j\).
- $\alpha$: Suma de los parámetros de suavizado.

---

### 1.9.4. Bernoulli Naive Bayes

El **BernoulliNB** implementa el Naive Bayes para datos distribuidos según la distribución de **Bernoulli**, es decir, características binarias. Este clasificador es útil para datos donde cada característica es binaria, como la **presencia o ausencia de palabras** en la clasificación de texto.

#### Regla de Decisión de Bernoulli Naive Bayes

La regla de decisión para el clasificador Bernoulli Naive Bayes es:

$$
P(x_i | y) = P(x_i = 1 | y)^{x_i} \cdot (1 - P(x_i = 1 | y))^{1 - x_i}
$$

#### Aplicación de Bernoulli Naive Bayes

- **$P(x_i = 1 | y)$**: Probabilidad de que la característica \(x_i\) ocurra dado \(y\).
- **$x_i$**: Valor binario que indica si la característica está presente (1) o ausente (0).
- Este clasificador penaliza explícitamente la no ocurrencia de una característica, lo que lo diferencia del Naive Bayes multinomial.

---

### 1.9.5. Categorical Naive Bayes

El **CategoricalNB** implementa el Naive Bayes para datos categóricos. Este clasificador es útil cuando cada característica tiene una distribución categórica, es decir, se representa como una de varias categorías posibles.

#### Fórmula de Categorical Naive Bayes

La probabilidad de la categoría $t$ en la característica $i$ dado $y$ es estimada como:

$$
P(x_i = t | y) = \frac{N_{t, i, c} + \alpha}{N_c + \alpha n_i}
$$

#### Parámetros de la Fórmula

- $N_{t, i, c}$: Número de veces que la categoría \(t\) aparece en la característica \(i\) en la clase \(y\).
- $N_c$: Número total de muestras en la clase \(y\).
- $\alpha$: Parámetro de suavizado que evita probabilidades de cero.
- $n_i$: Número de categorías posibles para la característica \(i\).

---

### 1.9.6. Modelo Naive Bayes fuera de memoria

Los modelos **Naive Bayes** pueden utilizarse para resolver problemas de clasificación a gran escala en los que el conjunto de entrenamiento no cabe en la memoria.

- Clasificadores como **MultinomialNB**, **BernoulliNB** y **GaussianNB** exponen un método `partial_fit`, que permite ajustar el modelo de forma incremental.

#### Código de partial_fit

Aquí tienes un ejemplo de uso del método `partial_fit` para un modelo GaussianNB:

```python
gnb.partial_fit(X_train, y_train, classes=[0, 1, 2])
```

---

### En Resumen

##### 1.	Naive Bayes Overview:
- Explicación breve de la suposición Naive, el teorema de Bayes y la estimación del Máximo A Posteriori (MAP).

##### 2.Gaussian Naive Bayes:
- Se enfocan en cómo se asumen distribuciones gaussianas para las características y se estiman los parámetros con máxima verosimilitud.

##### 3.	Multinomial Naive Bayes:
- Utilizado para la clasificación de texto; explica las distribuciones multinomiales y técnicas de suavizado (Laplace, Lidstone).

##### 4.	Complement Naive Bayes:
- Útil para conjuntos de datos desequilibrados, utilizando estadísticas del complemento para mejorar el rendimiento.

---

### En Resumen

##### 5.	Bernoulli Naive Bayes:
- Ideal para vectores de características binarias, comúnmente usado en clasificación de texto con ocurrencias binarias.

##### 6.	Categorical Naive Bayes:
- Maneja características categóricas, utilizando distribuciones categóricas para calcular probabilidades.

##### 7.	Naive Bayes fuera de memoria:
- Explica el aprendizaje incremental con partial_fit para grandes conjuntos de datos que no caben en memoria de una sola vez.

---

### Un Poco de Historia: Máquinas de Soporte Vectorial (SVM)

- Las SVM fueron desarrolladas por Vladimir Vapnik y su equipo en los laboratorios AT&T a finales de los años 70 y durante los 80.
- El modelo fue presentado formalmente en la conferencia **COLT (Computational Learning Theory)** en 1992 por Vapnik y otros autores.
- Este avance marcó un hito importante al llevar la **formulación teórica** de las SVM hacia su **aplicación práctica** en problemas reales de reconocimiento de formas (*pattern recognition*).

---

### Clasificación Binaria Lineal en SVM

- **Objetivo**: Encontrar un hiperplano que separe dos clases de manera óptima.
- **Margen máximo**: Se maximiza la distancia entre las clases y el hiperplano.
- **Función de decisión**: Clasifica los puntos según su posición relativa al hiperplano.
- **Soporte vectorial**: Los puntos más cercanos al hiperplano que definen la frontera de separación.
- **Linealidad**: Adecuado para problemas donde las clases son separables linealmente.

---


### Imagen Clasificación Binaria Lineal en SVM

![Clasificacion Binaria Lineal](Imagen2.png)

---

## Cual es el mejor hiperplano separador?

![Clasificacion Binaria Lineal](Imagen3.png)

---

## Hiperplano que Maximiza el Margen Geométrico

- La idea central de las **SVM de margen máximo** consiste en seleccionar el hiperplano que maximiza la distancia mínima (o **margen geométrico**) entre los ejemplos del conjunto de datos y el hiperplano.
  
- Solo los puntos que se encuentran en las fronteras (conocidos como **vectores soporte**) son considerados para definir el hiperplano óptimo.
  
- Este enfoque se justifica dentro de la **teoría del aprendizaje estadístico** y está alineado con el principio de **Minimización del Riesgo Estructural**.

- Referencia: [Hernández et al., 2004].

---

## Hiperplano que Maximiza el Margen Geométrico

![Hiperplano que Maximiza el Margen Geométrico](Imagen4.png)

---

## Clasificación Lineal

Supongamos dos vectores $x = (x, y)$ y $w = (w_1, w_2)$, y una constante $b$, la ecuación de la recta $y = ax + c$ se define como:

$$
x \cdot w + b = 0
$$

Expandiendo:

$$
(x, y) \cdot (w_1, w_2) + b = 0
$$

$$
xw_1 + yw_2 + b = 0
$$

Despejando para $y$:

$$
y = -\frac{w_1}{w_2}x - \frac{b}{w_2}
$$

---

## Función de Clasificación

La función de clasificación para una entrada $x_i$ se define como:

$$
f(x_i) = 
\begin{cases} 
+1 & \text{si } x \cdot w + b \geq 0 \\
-1 & \text{si } x \cdot w + b < 0
\end{cases}
$$

---

## Clasificacion No Lineal

![Clasificación No Lineal](Imagen5.png)

---

## Clasificación No-Lineal

- El aprendizaje de **separadores no lineales** se consigue mediante una **transformación no lineal** del espacio de atributos de entrada (input space) hacia un espacio de características (**feature space**) de dimensionalidad mucho mayor, donde es posible realizar una separación lineal.

- El uso de las denominadas **funciones núcleo** (*kernel functions*), que calculan el producto escalar de dos vectores en el espacio de características, permite trabajar de manera eficiente en dicho espacio sin la necesidad de calcular explícitamente las transformaciones de los ejemplos de aprendizaje.

- Referencia: [Hernández et al., 2004].

---

## Kernel

- Un **kernel** es una función que devuelve el resultado del **producto punto** entre dos vectores, realizado en un espacio dimensional diferente al espacio original donde se encuentran los vectores.

- El **producto punto** entre dos vectores $a$ y $b$ se define como:

$$
a \cdot b = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \dots + a_n b_n
$$

- Esta transformación permite realizar operaciones en un espacio de características de mayor dimensionalidad sin necesidad de calcular explícitamente las coordenadas de los vectores en el nuevo espacio.

---

## Kernel Lineal

![Kernel Lineal](Imagen6.png)

---

## Kernel Polimonico

![Kernel Polimonico](Imagen7.png)

---

## Kernel Gaussiano

![Kernel Gaussiano](Imagen8.png)

---

## Algoritmo para un SVM

1. Definir el hiperplano $X\beta = 0$.
2. Transformar los datos utilizando una **función de núcleo** (*kernel*).
3. Seleccionar un hiperplano que maximice el **margen geométrico**.
4. Como la posibilidad de una separación perfecta es baja, es necesario permitir cierta **holgura** para el margen, permitiendo que algunos puntos estén en el lado equivocado del margen.
5. Determinar los **hiperparámetros óptimos** (por ejemplo, $\gamma$, penalización).
6. Evaluar el modelo.

---

## Ventajas de SVM

- **Alta dimensionalidad**: SVM es eficaz en espacios de alta dimensión, como en la clasificación de documentos y el análisis de sentimientos, donde la dimensionalidad puede ser extremadamente grande.
  
- **Eficiencia de memoria**: Solo un subconjunto de los puntos de entrenamiento (vectores soporte) se utiliza en el proceso de decisión para asignar nuevos miembros, lo que reduce la memoria requerida.
  
- **Versatilidad**: La capacidad de aplicar diferentes **funciones núcleo** (*kernel*) permite mayor flexibilidad en los límites de decisión, mejorando el rendimiento de la clasificación, especialmente en problemas no lineales.

---

## Desventajas de SVM

- **Selección de parámetros del kernel**: Las SVM son muy sensibles a la elección de los parámetros del **kernel**. En espacios de características de alta dimensión con pocas muestras, los vectores soporte son menos efectivos, lo que puede llevar a un rendimiento de clasificación deficiente al agregar nuevas muestras.

- **No probabilístico**: SVM no proporciona una interpretación probabilística directa para la pertenencia a un grupo. El clasificador solo coloca los objetos por encima o por debajo de un hiperplano de clasificación. Sin embargo, una métrica para determinar la "efectividad" de la clasificación es la distancia del nuevo punto al límite de decisión.

---

## SVM vs. Redes Neuronales (NN)

### Similaridades:
- Ambos algoritmos son **paramétricos**, dependen del parámetro de costo (C) y de la función **kernel**.
- Ambos pueden aproximar funciones de decisión **no lineales**.
- Clasifican con una **precisión comparable**.

---

## SVM vs. Redes Neuronales (NN)

### Diferencias:
- Una **red neuronal profunda** tiene una complejidad mayor que una SVM con el mismo número de parámetros.
- SVM identifica de forma fiable el **límite de decisión** usando vectores soporte, mientras que NN requiere procesar **toda la data de entrenamiento**.
- SVM requiere **menos tiempo de procesamiento** que NN.
- SVM son más confiables y garantizan la **convergencia a un mínimo global**, independientemente de la configuración inicial.

---

## Aplicaciones de SVM

- **Problema de la doble espiral**: Un problema artificial que consiste en aprender a distinguir dos áreas que definen espirales en un plano bidimensional.
  
- **Categorización o clasificación de textos**: Usado para la organización automática de documentos y filtrado de documentos, como el filtrado de mensajes no deseados o **spam**.

- **Modelo basado en bag-of-words**: Representa documentos utilizando vectores de palabras, conocido como el modelo de **bolsa de palabras**.

---

### Ejemplos Scikit-learn: Máquinas de Soporte Vectorial (SVM)

Las **Máquinas de Soporte Vectorial (SVM)** son un conjunto de métodos de aprendizaje supervisado utilizados para clasificación, regresión y detección de outliers.

#### Ventajas de las SVM:

- Efectivas en espacios de alta dimensionalidad.
- Efectivas incluso cuando el número de dimensiones es mayor que el número de muestras.
- Usan un subconjunto de puntos de entrenamiento en la función de decisión (vectores de soporte), por lo que también son eficientes en memoria.
- Versátiles: Se pueden especificar diferentes funciones de Kernel. Los kernels comunes están disponibles, pero también es posible especificar kernels personalizados.

#### Desventajas de las SVM:

- Si el número de características es mucho mayor que el número de muestras, evitar el sobreajuste al elegir las funciones Kernel y el término de regularización es crucial.
- Las SVM no proporcionan estimaciones de probabilidad directamente; se calculan utilizando una costosa validación cruzada de cinco pliegues.

---

### Clasificación con SVM

Las clases **SVC**, **NuSVC** y **LinearSVC** pueden realizar clasificación binaria y multiclase en un conjunto de datos.

```python
from sklearn import svm
X = [[0, 0], [1, 1]]
y = [0, 1]
clf = svm.SVC()
clf.fit(X, y)
clf.predict([[2., 2.]])
```

- Las SVM utilizan un subconjunto de los datos de entrenamiento llamados vectores de soporte.
- Propiedades como los vectores de soporte se pueden obtener con los atributos support_vectors_, support_ y n_support_.

---

### 1.4.1.1 Clasificación multiclase

Las clases SVC y NuSVC implementan la estrategia de “uno contra uno” para la clasificación multiclase.

```python
X = [[0], [1], [2], [3]]
Y = [0, 1, 2, 3]
clf = svm.SVC(decision_function_shape='ovo')
clf.fit(X, Y)
clf.decision_function_shape = "ovr"
```

- Por otro lado, LinearSVC implementa la estrategia “uno contra el resto” y entrena n modelos para las n clases.

---

### 1.4.1.2 Puntuaciones y probabilidades

El método decision_function de SVC y NuSVC proporciona puntuaciones por clase para cada muestra.

- Si se establece probability=True, se habilitan las estimaciones de probabilidad.

Para estimaciones más precisas, se recomienda utilizar decision_function en lugar de predict_proba.

---

### 1.4.2. Regresión con SVM

El método de clasificación de soporte vectorial se puede extender para resolver problemas de regresión, conocido como Regresión de Soporte Vectorial (SVR).

```python
from sklearn import svm
X = [[0, 0], [2, 2]]
y = [0.5, 2.5]
regr = svm.SVR()
regr.fit(X, y)
regr.predict([[1, 1]])
```

---

### 1.4.3. Estimación de Densidad y Detección de Novedades

La clase **OneClassSVM** implementa una SVM de una sola clase (**One-Class SVM**) que se utiliza para la detección de outliers (valores atípicos) o anomalías en los datos.

#### Propósito de OneClassSVM

- Detecta datos que no siguen el patrón general de un conjunto de datos.
- Es especialmente útil en situaciones donde los datos de entrenamiento no contienen ejemplos de outliers.

#### Ejemplo de Detección de Outliers

```python
from sklearn import svm
import numpy as np

# Generar datos de ejemplo
X = 0.3 * np.random.randn(100, 2)
X_train = np.r_[X + 2, X - 2]

# Entrenar el modelo OneClassSVM
clf = svm.OneClassSVM(gamma='auto').fit(X_train)

# Predecir si las muestras son outliers o no
y_pred_train = clf.predict(X_train)

# Resultados:
print("Predicción de entrenamiento:", y_pred_train)
```

---

### 1.4.4. Complejidad

Las **Máquinas de Soporte Vectorial (SVM)** son herramientas poderosas, pero sus requisitos de cómputo y almacenamiento aumentan rápidamente con el número de vectores de entrenamiento.

#### Problema de Programación Cuadrática (QP)

El núcleo de una SVM es un problema de **programación cuadrática (QP)**, que separa los **vectores de soporte** del resto de los datos de entrenamiento.

- Este problema es computacionalmente costoso debido al número de vectores de entrenamiento y la dimensionalidad del espacio de características.

---

### Escalabilidad de SVM con libsvm

- El solucionador de **QP** utilizado por la implementación basada en **libsvm** escala entre:

$$
\mathcal{O}(n_{\text{training}}^2)
$$

y 

$$
\mathcal{O}(n_{\text{training}}^3)
$$

- dependiendo de qué tan eficientemente se use la caché de **libsvm** en la práctica (esto depende del conjunto de datos).

### Casos de Datos Dispersos

Si los datos son muy dispersos, este costo de complejidad debe reemplazarse por el **número promedio de características no nulas** en un vector de muestra.

---

## Eficiencia en el Caso Lineal

Para el caso lineal, el algoritmo utilizado en **LinearSVC** (implementado por **liblinear**) es mucho más eficiente que su contraparte basada en **libsvm**.

- Puede **escalar casi linealmente** a millones de muestras y/o características, lo que lo hace más adecuado para grandes conjuntos de datos.

```python
from sklearn import svm
X = [[0, 0], [2, 2]]
y = [0.5, 2.5]

# Ajuste del modelo utilizando LinearSVC
regr = svm.LinearSVC()
regr.fit(X, y)
```

---

### 1.4.5. Consejos para el uso práctico

#### Escalar los datos

- Las SVM no son invariantes al escalado, por lo que es altamente recomendable escalar los datos.

```python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
clf = make_pipeline(StandardScaler(), SVC())
```

Al utilizar **SVM** es importante tener en cuenta algunos aspectos prácticos para optimizar el rendimiento y evitar errores comunes.

#### Evitar la Copia de Datos

- Para **SVC**, **SVR**, **NuSVC** y **NuSVR**, si los datos no están en formato **C-contiguo** y de doble precisión, serán copiados antes de llamar a la implementación subyacente en **C**.
- Verifica si un array de **numpy** es **C-contiguo** inspeccionando su atributo `flags`.
- Para **LinearSVC** (y **LogisticRegression**), cualquier entrada pasada como array **numpy** será copiada y convertida a la representación interna de datos dispersos de **liblinear**.
  
---

### Tamaño de la Caché del Kernel

- Para SVC, SVR, NuSVC y NuSVR, el tamaño de la caché del kernel impacta significativamente en el tiempo de ejecución para problemas grandes.
- Si tienes suficiente RAM, se recomienda ajustar cache_size a un valor más alto, como 500MB o 1000MB.

```python
from sklearn import svm
clf = svm.SVC(cache_size=500)
```

#### Ajuste del Parámetro C

- El parámetro C tiene un valor predeterminado de 1, lo cual es razonable en general.
- Si tienes muchas observaciones con ruido, deberías disminuir C, ya que C menor implica más regularización.
- Para LinearSVC y LinearSVR, los resultados de predicción mejoran hasta un umbral, después del cual valores más altos de C ya no aportan beneficios.

```python
clf = svm.LinearSVC(C=0.1)
clf.fit(X, y)
```

---

### Control de Aleatoriedad

- Algunas implementaciones, como SVC y NuSVC, usan un generador de números aleatorios para barajar los datos durante la estimación de probabilidades (si probability=True).
- Controla la aleatoriedad con el parámetro random_state.

```python
clf = svm.SVC(random_state=42, probability=True)
```

---

### Kernels Personalizados

```python
import matplotlib.pyplot as plt
import numpy as np

from sklearn import datasets, svm
from sklearn.inspection import DecisionBoundaryDisplay

# Importamos algunos datos para trabajar
iris = datasets.load_iris()
X = iris.data[:, :2]  # solo tomamos las dos primeras características. Podríamos
# evitar este corte seleccionando un conjunto de datos bidimensional
Y = iris.target


def my_kernel(X, Y):
    """
    Creamos un kernel personalizado:

                 (2  0)
    k(X, Y) = X  (    ) Y.T
                 (0  1)
    """
    M = np.array([[2, 0], [0, 1.0]])
    return np.dot(np.dot(X, M), Y.T)


h = 0.02  # tamaño del paso en la malla

# Creamos una instancia de SVM y ajustamos nuestros datos.
clf = svm.SVC(kernel=my_kernel)
clf.fit(X, Y)

ax = plt.gca()
DecisionBoundaryDisplay.from_estimator(
    clf,
    X,
    cmap=plt.cm.Paired,
    ax=ax,
    response_method="predict",
    plot_method="pcolormesh",
    shading="auto",
)

# También trazamos los puntos de entrenamiento
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors="k")
plt.title("Clasificación de 3 clases usando SVM con kernel personalizado")
plt.axis("tight")
plt.show()
```

---

![Kernel Personalizado](Imagen9.png)

---

### Conclusiones

- **¿Cómo se define el Teorema de Bayes?**
  
- **¿En qué consiste la clasificación Bayesiana?**
  
- **¿Cuáles son las ventajas y desventajas de Naive Bayes (NB)?**
  
- **¿Cómo funciona el algoritmo SVM?**
  
- **¿Cuáles funciones núcleo (kernel) se pueden utilizar?**
  
- **¿Cuáles son las ventajas y desventajas de SVM?**
  
- **Aplicaciones** de SVM y Naive Bayes.

---

## Referencias

1. EMC Education Services (2015). *Data Science and Big Data Analytics*.
2. Hernández, J., Ramírez, M. J., Ferri, C. (2004). *Introducción a la minería de datos*.
3. Material de Clase Mailiu: <m.dazpea@uandresbello.edu>
4. **H. Zhang** (2004). *The optimality of Naive Bayes*. Proc. FLAIRS.
5. **Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R.** (2003). *Tackling the poor assumptions of naive Bayes text classifiers*. In ICML (Vol. 3, pp. 616-623).
6. **C.D. Manning, P. Raghavan and H. Schütze** (2008). *Introduction to Information Retrieval*. Cambridge University Press.
7. **McCallum, A., & Nigam, K.** (1998). *A comparison of event models for Naive Bayes text classification*. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.
8. **Metsis, V., Androutsopoulos, I., & Paliouras, G.** (2006). *Spam filtering with Naive Bayes – Which Naive Bayes?* 3rd Conf. on Email and Anti-Spam (CEAS).
9. **Raphael A. Finkel and J.L. Bentley** (1979). *Quad Trees: A Data Structure for Retrieval on Composite Keys*. Stanford University, [CS-TR-79-773](http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf).

---

## Anexos: "Validación Cruzada: Evaluación del Rendimiento del Estimador"

![Cross_Validation](Cross_Validation1.png)

---

## **Introducción a la Validación Cruzada**

- La validación cruzada se utiliza para evaluar el rendimiento de un modelo al dividir los datos en conjuntos de entrenamiento y prueba.

- El **overfitting** ocurre cuando un modelo aprende el ruido en los datos en lugar de la señal real.
    - Esto conduce a una precisión perfecta en los datos de entrenamiento, pero un mal rendimiento en datos no vistos.

- En scikit-learn, una división aleatoria en conjuntos de entrenamiento y prueba se puede calcular con la función train_test_split

- Vamos a cargar el conjunto de datos iris para ajustar una máquina de soporte vectorial lineal (SVM) en él:

### División de Conjuntos en Scikit-learn

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn import svm

# Cargar el conjunto de datos Iris
X, y = datasets.load_iris(return_X_y=True)

# Ver la forma de los datos
X.shape, y.shape
```

---

## División de Conjuntos en Scikit-learn

- Ahora podemos tomar una muestra del conjunto de entrenamiento, reservando el 40% de los datos para la evaluación de nuestro clasificador.

```python
# Dividir los datos en conjuntos de entrenamiento y prueba, 
# reservando el 40% de los datos para la prueba (test_size=0.4).
# El parámetro random_state asegura que la división sea reproducible.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=0)

# Mostrar la forma (dimensiones) del conjunto de entrenamiento (X e y)
X_train.shape, y_train.shape

# Mostrar la forma (dimensiones) del conjunto de prueba (X e y)
X_test.shape, y_test.shape

# Entrenar un clasificador de máquina de soporte vectorial (SVM) con un kernel lineal.
# El parámetro C=1 regula el margen del clasificador.
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)

# Calcular la puntuación de precisión del modelo en el conjunto de prueba.
clf.score(X_test, y_test)
```

---

## Riesgo de Sobreajuste al Ajustar Hiperparámetros en Modelos

- Al ajustar los hiperparámetros, como el valor de C en una SVM, existe el riesgo de sobreajuste en el conjunto de prueba. 

- Esto ocurre porque los parámetros pueden ajustarse hasta que el modelo rinda de manera óptima en el conjunto de prueba, lo que puede provocar que el modelo se ajuste demasiado a esos datos y no generalice bien con datos nuevos. 

- Este problema se conoce como fuga de conocimiento del conjunto de prueba hacia el modelo.

### La Solución: Conjunto de Validación

- Para mitigar este riesgo, una parte adicional del conjunto de datos se reserva como conjunto de validación. 

- El modelo se entrena en el conjunto de entrenamiento y luego se evalúa en el conjunto de validación para ajustar los hiperparámetros. 

- Después de obtener buenos resultados en el conjunto de validación, se realiza una evaluación final en el conjunto de prueba.

### Problema de la Partición de Datos

- Pero dividir los datos en tres conjuntos (entrenamiento, validación y prueba) reduce drásticamente el número de muestras disponibles para entrenar el modelo, lo que podría afectar su precisión. 

- Además, los resultados pueden depender de una selección aleatoria particular de los conjuntos de entrenamiento y validación.

- Una solución a este problema es un procedimiento llamado validación cruzada (abreviado como CV). Se debe mantener un conjunto de prueba para la evaluación final, pero el conjunto de validación ya no es necesario cuando se utiliza la validación cruzada. 


---

## Validacion Cruzada K-Fold

- En el enfoque básico, llamado validación cruzada k-fold, el conjunto de entrenamiento se divide en k subconjuntos más pequeños. El siguiente procedimiento se sigue para cada uno de los k “folds”:
  1. Un modelo se entrena utilizando k-1 de los subconjuntos como datos de entrenamiento;
  2. El modelo resultante se valida en la parte restante de los datos (es decir, se utiliza como conjunto de prueba para calcular una métrica de rendimiento, como la precisión).

- La métrica de rendimiento reportada por la validación cruzada k-fold es el promedio de los valores calculados en el bucle. 

- Este enfoque puede ser computacionalmente costoso, pero no desperdicia demasiados datos (como ocurre al fijar arbitrariamente un conjunto de validación).

![Cross_Validation](Cross_Validation2.png)

---

## Cálculo de métricas con validación cruzada

- La forma más simple de utilizar la validación cruzada es llamar a la función auxiliar cross_val_score en el estimador y el conjunto de datos.

- El siguiente ejemplo demuestra cómo estimar la precisión de una máquina de soporte vectorial con kernel lineal en el conjunto de datos iris, dividiendo los datos, ajustando un modelo y calculando la puntuación 5 veces consecutivas (con divisiones diferentes cada vez):

```python
# Importar la función cross_val_score para realizar validación cruzada
from sklearn.model_selection import cross_val_score

# Crear un clasificador SVM con un kernel lineal y un estado aleatorio fijo para reproducibilidad
clf = svm.SVC(kernel='linear', C=1, random_state=42)

# Realizar validación cruzada con 5 particiones (cv=5)
scores = cross_val_score(clf, X, y, cv=5)

# Mostrar las puntuaciones de precisión obtenidas en cada partición
scores
```

- Resultado array([0.96..., 1. , 0.96..., 0.96..., 1. ])

- La puntuación media y la desviación estándar se dan de la siguiente manera:

```python
# Imprimir la precisión promedio y la desviación estándar de las puntuaciones obtenidas
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
```
- 0.98 accuracy with a standard deviation of 0.02

---

- Por defecto, la puntuación calculada en cada iteración de la validación cruzada (CV) es el método score del estimador. Es posible cambiar esto utilizando el parámetro scoring:

```python
# Importar la métrica de evaluación
from sklearn import metrics

# Realizar validación cruzada con 5 particiones (cv=5) utilizando la métrica F1 macro
scores = cross_val_score(
    clf, X, y, cv=5, scoring='f1_macro')

# Mostrar las puntuaciones F1 obtenidas en cada partición
scores
```

- Resultado: array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])

- Ver el parámetro de evaluación: The scoring parameter: definir las reglas de evaluación del modelo para más detalles. En el caso del conjunto de datos Iris, las muestras están equilibradas entre las clases objetivo, por lo tanto, la exactitud (accuracy) y el puntaje F1 son casi iguales.

- Cuando el argumento cv es un número entero, cross_val_score utiliza las estrategias KFold o StratifiedKFold por defecto, siendo la última usada si el estimador proviene de ClassifierMixin.

---

## La función cross_validate y la evaluación de múltiples métricas

- La función cross_validate se diferencia de cross_val_score en dos aspectos:

	1.	Permite especificar múltiples métricas para la evaluación.
	2.	Devuelve un diccionario que contiene los tiempos de ajuste (fit-times), tiempos de puntuación (score-times) (y opcionalmente los puntajes de entrenamiento, estimadores ajustados, índices de división de entrenamiento-prueba) además del puntaje de prueba.

- Para la evaluación de una sola métrica, donde el parámetro scoring es una cadena, función callable o None, las claves serán: ['test_score', 'fit_time', 'score_time'].

- Y para la evaluación de múltiples métricas, el valor devuelto es un diccionario con las siguientes claves: ['test_<nombre_scorer1>', 'test_<nombre_scorer2>', 'test_<nombre_scorer...>', 'fit_time', 'score_time'].

- El parámetro return_train_score está configurado en False por defecto para ahorrar tiempo de computación. 

- Para evaluar los puntajes en el conjunto de entrenamiento también, debes establecerlo en True. También puedes conservar el estimador ajustado en cada conjunto de entrenamiento estableciendo return_estimator=True. 

- De manera similar, puedes establecer return_indices=True para retener los índices de entrenamiento y prueba utilizados para dividir el conjunto de datos en conjuntos de entrenamiento y prueba para cada división de validación cruzada.

---

## La función cross_validate y la evaluación de múltiples métricas

Las múltiples métricas pueden especificarse como una lista, tupla o conjunto de nombres de evaluadores (scorers) predefinidos:

```python
# Importar la función cross_validate para realizar validación cruzada con múltiples métricas
from sklearn.model_selection import cross_validate

# Importar la métrica de evaluación recall_score
from sklearn.metrics import recall_score

# Definir las métricas de evaluación que se utilizarán en la validación cruzada
scoring = ['precision_macro', 'recall_macro']

# Crear un clasificador SVM con un kernel lineal y un estado aleatorio fijo para reproducibilidad
clf = svm.SVC(kernel='linear', C=1, random_state=0)

# Realizar validación cruzada con las métricas definidas
scores = cross_validate(clf, X, y, scoring=scoring)

# Mostrar las claves del diccionario de resultados ordenadas
sorted(scores.keys())

# Mostrar las puntuaciones de recall macro obtenidas en cada partición
scores['test_recall_macro']
```

---

## La función cross_validate y la evaluación de múltiples métricas

- O como un diccionario que mapea el nombre del evaluador (scorer) a una función de evaluación (scoring) predefinida o personalizada:

```python
# Importar la función make_scorer para crear un evaluador personalizado
from sklearn.metrics import make_scorer

# Definir las métricas de evaluación que se utilizarán en la validación cruzada
# 'prec_macro' utiliza la métrica de precisión macro predefinida
# 'rec_macro' utiliza un evaluador personalizado para la métrica de recall macro
scoring = {'prec_macro': 'precision_macro',
           'rec_macro': make_scorer(recall_score, average='macro')}

# Realizar validación cruzada con las métricas definidas, utilizando 5 particiones (cv=5)
# y devolviendo las puntuaciones del conjunto de entrenamiento
scores = cross_validate(clf, X, y, scoring=scoring,
                        cv=5, return_train_score=True)

# Mostrar las claves del diccionario de resultados ordenadas
sorted(scores.keys())

# Mostrar las puntuaciones de recall macro obtenidas en el conjunto de entrenamiento en cada partición
scores['train_rec_macro']
```

---

## La función cross_validate y la evaluación de una unica métrica


- Aquí tienes un ejemplo de uso de cross_validate utilizando una única métrica:

```python
# Realizar validación cruzada con la métrica de precisión macro, utilizando 5 particiones (cv=5)
# y devolviendo los estimadores ajustados para cada partición
scores = cross_validate(clf, X, y,
                        scoring='precision_macro', cv=5,
                        return_estimator=True)

# Mostrar las claves del diccionario de resultados ordenadas
sorted(scores.keys())
```

---

## Obtención de predicciones mediante validación cruzada

- La función cross_val_predict tiene una interfaz similar a cross_val_score, pero devuelve, para cada elemento en la entrada, la predicción que se obtuvo para ese elemento cuando estaba en el conjunto de prueba. 

- Solo se pueden utilizar estrategias de validación cruzada que asignen todos los elementos a un conjunto de prueba exactamente una vez (de lo contrario, se generará una excepción).

- Nota sobre el uso inapropiado de cross_val_predict:
  1. El resultado de cross_val_predict puede ser diferente de aquellos obtenidos utilizando cross_val_score, ya que los elementos se agrupan de diferentes maneras. 
  2. La función cross_val_score toma un promedio sobre los pliegues de validación cruzada, mientras que cross_val_predict simplemente devuelve las etiquetas (o probabilidades) de varios modelos distintos sin diferenciarlos. 
  3. Por lo tanto, cross_val_predict no es una medida adecuada del error de generalización.

---

## Ejemplos:

### K-Fold

```python
## Ejemplo de K-Fold con K=5

from sklearn.model_selection import KFold
import numpy as np

# Crear un conjunto de datos de ejemplo
X = np.arange(20)  # Datos de ejemplo

# Crear el objeto KFold con 5 particiones, sin barajar los datos y sin estado aleatorio fijo
kf = KFold(n_splits=5, shuffle=False, random_state=None)

# Iterar sobre cada partición (fold)
for fold, (train_index, test_index) in enumerate(kf.split(X)):
    print(f"Fold {fold+1}")
    print(f"Entrenamiento: {train_index}, Prueba: {test_index}\n")
```

---

- Podemos visualizar cómo se divide el conjunto de datos en cada iteración:

```python
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
import numpy as np

# Crear un conjunto de datos de ejemplo
X = np.arange(20)  # Datos de ejemplo

# Crear el objeto KFold con 5 particiones, barajando los datos y con un estado aleatorio fijo
kf = KFold(n_splits=5, shuffle=False, random_state=None)

# Crear una figura y un eje para la visualización
fig, ax = plt.subplots()

# Iterar sobre cada partición (fold)
for i, (train_index, test_index) in enumerate(kf.split(X)):
    # Crear listas de índices para los conjuntos de entrenamiento y prueba
    y_train = [i + 0.5] * len(train_index)
    y_test = [i + 0.5] * len(test_index)
    
    # Graficar los índices de entrenamiento y prueba
    ax.scatter(train_index, y_train, c='blue', marker='s', s=100, label='Entrenamiento' if i == 0 else "")
    ax.scatter(test_index, y_test, c='red', marker='o', s=100, label='Prueba' if i == 0 else "")

# Configurar etiquetas y título del gráfico
ax.set_xlabel('Índice de muestra')
ax.set_yticks([i + 0.5 for i in range(5)])
ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])
ax.set_title('Visualización de K-Fold Cross-Validation')
ax.legend()
```

---

## Validación Cruzada con Grupos

- Group K-Fold Cross-Validation
  - GroupKFold asegura que las muestras del mismo grupo no aparezcan en ambos conjuntos de entrenamiento y prueba.
  - Útil cuando las muestras dentro de un grupo están relacionadas o son dependientes.

Ejemplo con Grupos

```python
# Crear un conjunto de datos de ejemplo
X = np.arange(10)  # Datos de ejemplo
y = np.random.randint(0, 2, size=10)  # Etiquetas de ejemplo (0 o 1)
groups = np.array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5])  # Grupos de ejemplo

# Crear el objeto GroupKFold con 5 particiones
gkf = GroupKFold(n_splits=5)

# Iterar sobre cada partición (fold)
for train_index, test_index in gkf.split(X, y, groups):
    print(f"Entrenamiento: {train_index}, Prueba: {test_index}")
```

---

## Visualización de GroupKFold

- Group K-Fold Cross-Validation se utiliza cuando tus datos están organizados en grupos y quieres asegurarte de que las muestras de un mismo grupo no aparezcan en ambos conjuntos de entrenamiento y prueba durante la validación cruzada.

```python
from sklearn.model_selection import GroupKFold
import numpy as np

# Crear un conjunto de datos de ejemplo
X = np.arange(10)  # Datos de ejemplo
y = np.random.randint(0, 2, size=10)  # Etiquetas de ejemplo (0 o 1)
groups = np.array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5])  # Grupos de ejemplo

# Crear el objeto GroupKFold con 5 particiones
gkf = GroupKFold(n_splits=5)

# Iterar sobre cada partición (fold)
for train_index, test_index in gkf.split(X, y, groups):
    print(f"Entrenamiento: {train_index}, Prueba: {test_index}")

    import matplotlib.pyplot as plt
    from sklearn.model_selection import GroupKFold
    import numpy as np

    # Crear un conjunto de datos de ejemplo
    X = np.arange(10)  # Datos de ejemplo
    groups = np.array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5])  # Grupos de ejemplo

    # Crear el objeto GroupKFold con 5 particiones
    gkf = GroupKFold(n_splits=5)

    # Crear una figura y un eje para la visualización
    fig, ax = plt.subplots()

    # Iterar sobre cada partición (fold)
    for i, (train_index, test_index) in enumerate(gkf.split(X, groups=groups)):
        # Crear listas de índices para los conjuntos de entrenamiento y prueba
        y_train = [i + 0.5] * len(train_index)
        y_test = [i + 0.5] * len(test_index)
        
        # Graficar los índices de entrenamiento y prueba
        ax.scatter(train_index, y_train, c='blue', marker='s', s=100, label='Entrenamiento' if i == 0 else "")
        ax.scatter(test_index, y_test, c='red', marker='o', s=100, label='Prueba' if i == 0 else "")

    # Configurar etiquetas y título del gráfico
    ax.set_xlabel('Índice de muestra')
    ax.set_yticks([i + 0.5 for i in range(5)])
    ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])
    ax.set_title('Visualización de GroupKFold')
    ax.legend()

    # Mostrar el gráfico
    plt.show()
```

---

## Stratified K-Fold Cross-Validation


Concepto

	•	Stratified K-Fold mantiene la proporción de clases en cada fold.
	•	Es especialmente útil en conjuntos de datos desequilibrados.
	•	Garantiza que cada fold sea una representación adecuada del conjunto completo.


```python
from sklearn.model_selection import StratifiedKFold
import numpy as np

# Crear un conjunto de datos de ejemplo
X = np.ones((50, 1))
y = np.hstack((np.zeros(45), np.ones(5)))  # 45 ceros y 5 unos

# Crear el objeto StratifiedKFold con 5 particiones, barajando los datos y con un estado aleatorio fijo
skf = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)

# Iterar sobre cada partición (fold)
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
    print(f"Fold {fold+1}")
    # Mostrar la distribución de las clases en el conjunto de entrenamiento
    print(f"Distribución en entrenamiento: {np.bincount(y[train_index].astype(int))}")
    # Mostrar la distribución de las clases en el conjunto de prueba
    print(f"Distribución en prueba: {np.bincount(y[test_index].astype(int))}\n")
```

---

## Visualizacion: Stratified K-Fold Cross-Validation

- StratifiedGroupKFold es un esquema de validación cruzada que combina tanto StratifiedKFold como GroupKFold.
- La idea es intentar preservar la distribución de clases en cada división, manteniendo al mismo tiempo cada grupo dentro de una única división. 
- Esto puede ser útil cuando tienes un conjunto de datos desequilibrado, de modo que usar solo GroupKFold podría producir divisiones sesgadas.

```python
    import matplotlib.pyplot as plt
    from sklearn.model_selection import StratifiedKFold
    import numpy as np

    # Crear un conjunto de datos de ejemplo
    X = np.arange(50)
    y = np.hstack((np.zeros(45), np.ones(5)))

    # Crear el objeto StratifiedKFold con 5 particiones, barajando los datos y con un estado aleatorio fijo
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Crear una figura y un eje para la visualización
    fig, ax = plt.subplots()

    # Iterar sobre cada partición (fold)
    for i, (train_index, test_index) in enumerate(skf.split(X, y)):
        # Crear listas de índices para los conjuntos de entrenamiento y prueba
        y_train = [i + 0.5] * len(train_index)
        y_test = [i + 0.5] * len(test_index)
        
        # Graficar los índices de entrenamiento y prueba
        ax.scatter(train_index, y_train, c='blue', marker='s', s=10, label='Entrenamiento' if i == 0 else "")
        ax.scatter(test_index, y_test, c='red', marker='o', s=10, label='Prueba' if i == 0 else "")

    # Configurar etiquetas y título del gráfico
    ax.set_xlabel('Índice de muestra')
    ax.set_yticks([i + 0.5 for i in range(5)])
    ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])
    ax.set_title('Visualización de Stratified K-Fold')
    ax.legend()

    # Mostrar el gráfico
    plt.show()
```

---

## Preguntas y Respuestas

### Pregunta 1

- P: ¿Qué es el sobreajuste (overfitting) y por qué es perjudicial para un modelo de aprendizaje automático?

- R:

	- El sobreajuste ocurre cuando un modelo aprende los detalles y el ruido en los datos de entrenamiento hasta el punto de perjudicar su rendimiento en datos nuevos.

	- Es perjudicial porque el modelo no generaliza bien a datos no vistos, lo que significa que su rendimiento en el mundo real será pobre.

### Pregunta 2

- P: ¿Por qué es importante dividir los datos en conjuntos de entrenamiento y prueba?

- R:

	- Dividir los datos permite evaluar cómo generaliza el modelo a datos no vistos.
  - El conjunto de entrenamiento se utiliza para ajustar el modelo, mientras que el conjunto de prueba evalúa su rendimiento real.

Riesgo de Sobreajuste al Ajustar Hiperparámetros
  - Al ajustar hiperparámetros, como el valor de C en una SVM, existe el riesgo de sobreajuste si se evalúa repetidamente en el conjunto de prueba.

### Pregunta 3

- P: ¿Cómo podemos evitar el sobreajuste al ajustar hiperparámetros?

R:
  - Utilizando un conjunto de validación separado o aplicando validación cruzada.
  - La validación cruzada permite usar todos los datos para entrenamiento y validación sin sobreajustar al conjunto de prueba.

---

## Pregunta 4

- P: ¿Qué ventaja tiene la validación cruzada K-Fold sobre una única división entrenamiento/prueba?

- R:
  - La validación cruzada utiliza más datos para el entrenamiento y validación.
  - Proporciona una estimación más robusta y menos sesgada del rendimiento del modelo.

### Pregunta 5

- P: ¿Para qué sirve la función cross_validate y en qué se diferencia de cross_val_score?

- R:
  - cross_validate permite evaluar múltiples métricas simultáneamente y proporciona tiempos de ajuste y puntuación.
  - A diferencia de cross_val_score, que solo permite una métrica y devuelve las puntuaciones directamente.

### Pregunta 6

- P: ¿Es adecuado usar cross_val_predict para estimar el error de generalización? ¿Por qué?

- R:
  - No es adecuado porque las predicciones se obtienen de modelos entrenados en diferentes particiones.

  - No proporciona una métrica de rendimiento agregada y puede llevar a interpretaciones incorrectas del error.

  ---

## Introducción al Algoritmo k-NN

- ¿Qué es k-NN?
  -k-NN (k-Nearest Neighbors) es un algoritmo de aprendizaje supervisado utilizado tanto para clasificación como para regresión.
- -Se basa en la idea de que muestras similares están cerca en el espacio de características.

- ¿Cómo funciona?

	1.	Definición de k: Seleccionar el número de vecinos más cercanos a considerar.
	2.	Cálculo de distancias: Medir la distancia entre el punto de interés y todos los puntos en el conjunto de entrenamiento.
	3.	Identificación de vecinos: Seleccionar los k vecinos más cercanos según las distancias calculadas.
	4.	Predicción:
    - Clasificación: Asignar la clase más común entre los vecinos
    - Regresión: Calcular el promedio de los valores de los vecinos.

--- 

## Ventajas del Algoritmo k-NN

- Simplicidad: Fácil de entender e implementar.
- Flexibilidad: No hace suposiciones sobre la distribución de los datos
- Adaptabilidad: Puede manejar problemas de clasificación y regresión.

### Desventajas del Algoritmo k-NN

- Eficiencia Computacional:
  - Puede ser costoso en términos de tiempo de cálculo para conjuntos de datos grandes.
  -Requiere calcular la distancia a todos los puntos de entrenamiento para cada predicción.
  
- Almacenamiento:
  - Necesita almacenar todo el conjunto de entrenamiento en memoria
  - Sensibilidad a la Dimensionalidad:
  - Su rendimiento puede degradarse con datos de alta dimensión.



